@misc{xuConformalPredictionSet2022,
	title = {Conformal prediction set for time-series},
	url = {http://arxiv.org/abs/2206.07851},
	doi = {10.48550/arXiv.2206.07851},
	abstract = {When building either prediction intervals for regression
	            (with real-valued response) or prediction sets for
	            classification (with categorical responses), uncertainty
	            quantification is essential to studying complex machine
	            learning methods. In this paper, we develop Ensemble
	            Regularized Adaptive Prediction Set ({ERAPS}) to construct
	            prediction sets for time-series (with categorical responses),
	            based on the prior work of [Xu and Xie, 2021]. In particular,
	            we allow unknown dependencies to exist within features and
	            responses that arrive in sequence. Method-wise, { ERAPS} is a
	            distribution-free and ensemble-based framework that is
	            applicable for arbitrary classifiers. Theoretically, we bound
	            the coverage gap without assuming data exchangeability and
	            show asymptotic set convergence. Empirically, we demonstrate
	            valid marginal and conditional coverage by {ERAPS}, which
	            also tends to yield smaller prediction sets than competing
	            methods.},
	number = {{arXiv}:2206.07851},
	publisher = {{arXiv}},
	author = {Xu, Chen and Xie, Yao},
	urldate = {2023-08-29},
	date = {2022-06-15},
	eprinttype = {arxiv},
	eprint = {2206.07851 [cs, stat]},
	keywords = {conformal prediction, time series},
}

@article{yuReviewRecurrentNeural2019,
	title = {A review of recurrent neural networks: {LSTM} cells and network
	         architectures},
	volume = {31},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01199},
	doi = {10.1162/neco_a_01199},
	shorttitle = {A review of recurrent neural networks},
	abstract = {Recurrent neural networks ({RNNs}) have been widely adopted
	            in research areas concerned with sequential data, such as
	            text, audio, and video. However, {RNNs} consisting of sigma
	            cells or tanh cells are unable to learn the relevant
	            information of input data when the input gap is large. By
	            introducing gate functions into the cell structure, the long
	            short-term memory ({LSTM}) could handle the problem of
	            long-term dependencies well. Since its introduction, almost
	            all the exciting results based on {RNNs} have been achieved
	            by the {LSTM}. The {LSTM} has become the focus of deep
	            learning. We review the {LSTM} cell and its variants to
	            explore the learning capacity of the {LSTM} cell. Furthermore
	            , the {LSTM} networks are divided into two broad categories:
	            {LSTM} -dominated networks and integrated {LSTM} networks. In
	            addition, their various applications are discussed. Finally,
	            future research directions are presented for {LSTM} networks.
	            },
	pages = {1235--1270},
	number = {7},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun
	          },
	urldate = {2023-08-29},
	date = {2019-07-01},
	keywords = {deep learning},
}

@inproceedings{stankeviciuteConformalTimeseriesForecasting2021,
	title = {Conformal time-series forecasting},
	volume = {34},
	url = {
	       https://proceedings.neurips.cc/paper/2021/hash/312f1ba2a72318edaaa995a67835fad5-Abstract.html
	       },
	abstract = {Current approaches for multi-horizon time series forecasting
	            using recurrent neural networks ({RNNs}) focus on issuing
	            point estimates, which is insufficient for decision-making in
	            critical application domains where an uncertainty estimate is
	            also required. Existing approaches for uncertainty
	            quantification in { RNN}-based time-series forecasts are
	            limited as they may require significant alterations to the
	            underlying model architecture, may be computationally complex
	            , may be difficult to calibrate, may incur high sample
	            complexity, and may not provide theoretical guarantees on
	            frequentist coverage. In this paper, we extend the inductive
	            conformal prediction framework to the time-series forecasting
	            setup, and propose a lightweight algorithm to address all of
	            the above limitations, providing uncertainty estimates with
	            theoretical guarantees for any multi-horizon forecast
	            predictor and any dataset with minimal exchangeability
	            assumptions. We demonstrate the effectiveness of our approach
	            by comparing it with existing benchmarks on a variety of
	            synthetic and real-world datasets.},
	pages = {6216--6228},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Stankeviciute, Kamile and M. Alaa, Ahmed and van der Schaar,
	          Mihaela},
	urldate = {2023-08-29},
	date = {2021},
	keywords = {conformal prediction, time series},
}

@article{shaferTutorialConformalPrediction2008,
	title = {A tutorial on conformal prediction},
	volume = {9},
	number = {3},
	journaltitle = {Journal of Machine Learning Research},
	author = {Shafer, Glenn and Vovk, Vladimir},
	date = {2008},
	note = {{ISBN}: 1532-4435},
	keywords = {conformal prediction, machine learning},
}

@misc{kakushadze151TradingStrategies2018,
	location = {Rochester, {NY}},
	title = {151 trading strategies},
	url = {https://papers.ssrn.com/abstract=3247865},
	abstract = {We provide detailed descriptions, including over 550
	            mathematical formulas, for over 150 trading strategies across
	            a host of asset classes (and trading styles). This includes
	            stocks, options, fixed income, futures, {ETFs}, indexes,
	            commodities, foreign exchange, convertibles, structured
	            assets, volatility (as an asset class), real estate,
	            distressed assets, cash, cryptocurrencies, miscellany (such
	            as weather, energy, inflation) , global macro, infrastructure
	            , and tax arbitrage. Some strategies are based on machine
	            learning algorithms (such as artificial neural networks,
	            Bayes, k-nearest neighbors). We also give: source code for
	            illustrating out-of-sample backtesting with explanatory
	            notes; around 2,000 bibliographic references; and over 900
	            glossary, acronym and math definitions. The presentation is
	            intended to be descriptive and pedagogical. This is the
	            complete version of the book.},
	number = {3247865},
	author = {Kakushadze, Zura and Serur, Juan A.},
	urldate = {2023-10-21},
	date = {2018-08-17},
	langid = {english},
}

@book{jamesIntroductionStatisticalLearning2023,
	location = {Cham},
	title = {An introduction to statistical learning: with applications in
	         Python},
	isbn = {978-3-031-38746-3},
	series = {Springer texts in statistics},
	shorttitle = {An introduction to statistical learning},
	pagetotal = {607},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and
	          Tibshirani, Robert and Taylor, Jonathan E.},
	date = {2023},
	keywords = {statistics, machine learning},
}

@misc{kellyFinancialMachineLearning2023,
	location = {Rochester, {NY}},
	title = {Financial machine learning},
	url = {https://papers.ssrn.com/abstract=4520856},
	doi = {10.2139/ssrn.4520856},
	abstract = {We survey the nascent literature on machine learning in the
	            study of financial markets. We highlight the best examples of
	            what this line of research has to offer and recommend
	            promising directions for future research. This survey is
	            designed for both financial economists interested in grasping
	            machine learning tools, as well as for statisticians and
	            machine learners seeking interesting financial contexts where
	            advanced methods may be deployed.},
	number = {4520856},
	author = {Kelly, Bryan T. and Xiu, Dacheng},
	urldate = {2023-08-29},
	date = {2023-07-24},
	langid = {english},
	keywords = {machine learning, finance},
}

@misc{glausSeasonalityIngameItem2022,
	location = {Rochester, {NY}},
	title = {Seasonality of in-game item returns in the Steam Community
	         Market},
	url = {https://papers.ssrn.com/abstract=4314409},
	doi = {10.2139/ssrn.4314409},
	abstract = {Using Steam Community Market data for eight of the most
	            popular games from 2012 to 2021, this paper investigates the
	            existence of a sale effect, a market anomaly according to
	            which in-game item returns tend to be lower on days at the
	            beginning and higher on days in the aftermath of Steam store
	            seasonal sales relative to non-sale days. Motivated by the
	            growing interest in the ownership and trading of virtual
	            assets, this study is the first to empirically analyse
	            seasonality in the context of Steam sales across a broad
	            sample of games. Results show not only the existence of a
	            sale effect but also that the effect is robust to other
	            calendar anomalies, while slowly decreasing, has largely
	            persisted over time since the inception of the Community
	            Market and is neither driven by individual seasonal sales nor
	            by particular in-game item characteristics. An examination of
	            liquidity measures suggests that the sale effect may be
	            caused by higher trading volume on sale days versus non-sale
	            days. However, given high transaction costs, it is difficult
	            to implement trading strategies that can successfully exploit
	            the sale effect. },
	number = {4314409},
	author = {Glaus, Tim},
	urldate = {2023-08-29},
	date = {2022-03-19},
	langid = {english},
	keywords = {time series, finance, steam},
}

@article{hanReviewDeepLearning2021,
	title = {A review of deep learning models for time series prediction},
	volume = {21},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2019.2923982},
	abstract = {In order to approximate the underlying process of temporal
	            data, time series prediction has been a hot research topic
	            for decades. Developing predictive models plays an important
	            role in interpreting complex real-world elements. With the
	            sharp increase in the quantity and dimensionality of data,
	            new challenges, such as extracting deep features and
	            recognizing deep latent patterns, have emerged, demanding
	            novel approaches and effective solutions. Deep learning,
	            composed of multiple processing layers to learn with multiple
	            levels of abstraction, is, now, commonly deployed for
	            overcoming the newly arisen difficulties. This paper reviews
	            the state-of-the-art developments in deep learning for time
	            series prediction. Based on modeling for the perspective of
	            conditional or joint probability, we categorize them into
	            discriminative, generative, and hybrids models. Experiments
	            are implemented on both benchmarks and real-world data to
	            elaborate the performance of the representative deep
	            learning-based prediction methods. Finally, we conclude with
	            comments on possible future perspectives and ongoing
	            challenges with time series prediction.},
	pages = {7833--7848},
	number = {6},
	journaltitle = {{IEEE} Sensors Journal},
	author = {Han, Zhongyang and Zhao, Jun and Leung, Henry and Ma, King Fai
	          and Wang, Wei},
	date = {2021-03},
	note = {Conference Name: {IEEE} Sensors Journal},
	keywords = {time series, deep learning, machine learning},
}

@misc{angelopoulosGentleIntroductionConformal2022,
	title = {A gentle introduction to conformal prediction and
	         distribution-free uncertainty quantification},
	url = {http://arxiv.org/abs/2107.07511},
	doi = {10.48550/arXiv.2107.07511},
	abstract = {Black-box machine learning models are now routinely used in
	            high-risk settings, like medical diagnostics, which demand
	            uncertainty quantification to avoid consequential model
	            failures. Conformal prediction is a user-friendly paradigm
	            for creating statistically rigorous uncertainty
	            sets/intervals for the predictions of such models. Critically
	            , the sets are valid in a distribution-free sense: they
	            possess explicit, non-asymptotic guarantees even without
	            distributional assumptions or model assumptions. One can use
	            conformal prediction with any pre-trained model, such as a
	            neural network, to produce sets that are guaranteed to
	            contain the ground truth with a user-specified probability,
	            such as 90\%. It is easy-to-understand, easy-to-use, and
	            general, applying naturally to problems arising in the fields
	            of computer vision, natural language processing, deep
	            reinforcement learning, and so on. This hands-on introduction
	            is aimed to provide the reader a working understanding of
	            conformal prediction and related distribution-free
	            uncertainty quantification techniques with one self-contained
	            document. We lead the reader through practical theory for and
	            examples of conformal prediction and describe its extensions
	            to complex machine learning tasks involving structured
	            outputs, distribution shift, time-series, outliers, models
	            that abstain, and more. Throughout, there are many
	            explanatory illustrations, examples, and code samples in
	            Python. With each code sample comes a Jupyter notebook
	            implementing the method on a real-data example; the notebooks
	            can be accessed and easily run using our codebase.},
	number = {{arXiv}:2107.07511},
	publisher = {{arXiv}},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	urldate = {2023-08-29},
	date = {2022-12-07},
	eprinttype = {arxiv},
	eprint = {2107.07511 [cs, math, stat]},
	keywords = {conformal prediction, machine learning},
}
